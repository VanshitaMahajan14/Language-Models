{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cb19cKBfXUcl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "807efaae-fac5-4d1d-df31-2e6abaefadc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sj-Sv_h8n44-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c127e51-446d-4f63-bb79-f2055beb4706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBg-a_u2M3mF"
      },
      "source": [
        "## Preprocessing and Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UDZqnU25n7Ei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b257288-1f56-4887-e996-901622f1a5bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokenized train sentences: 41528\n",
            "Number of tokenized validation sentences: 11865\n",
            "Number of tokenized test sentences: 5932\n"
          ]
        }
      ],
      "source": [
        "corpus_path = '/content/drive/MyDrive/Auguste_Maquet.txt'\n",
        "\n",
        "# Load the entire corpus\n",
        "with open(corpus_path, \"r\") as f:\n",
        "    corpus = f.read()\n",
        "\n",
        "corpus_path = '/content/drive/MyDrive/Auguste_Maquet.txt'\n",
        "\n",
        "# Load the entire corpus\n",
        "with open(corpus_path, \"r\") as f:\n",
        "    corpus = f.read()\n",
        "\n",
        "def clean(data):\n",
        "    data = data.lower()\n",
        "    data = re.sub(r'\\n|\\s+', ' ', data)  # replace newline and multiple spaces with single space\n",
        "    data = re.sub(r\"[^a-zA-Z0-9\\s,.!?;:]\", \"\", data)\n",
        "    data = re.sub(r'[’‘]', '\\'', data)  # apostrophes\n",
        "    data = re.sub(r'[“”`\\' ]|[–—-]', ' ', data)  # quotes and dashes\n",
        "    data = data.strip()\n",
        "    return data\n",
        "\n",
        "corpus = clean(corpus)\n",
        "sentences = sent_tokenize(corpus)\n",
        "\n",
        "def add_start_and_end_tokens(sentences):\n",
        "  modified_sentences = []\n",
        "  for sentence in sentences:\n",
        "    sentence = sentence.lower()\n",
        "    tokenized_sentence = word_tokenize(sentence)\n",
        "    new_sentence = ['<S>'] + tokenized_sentence + ['</S>']\n",
        "    modified_sentences.append(new_sentence)\n",
        "  return modified_sentences\n",
        "\n",
        "sentences = add_start_and_end_tokens(sentences)\n",
        "\n",
        "# Calculate the adjusted split sizes\n",
        "train_size = int(0.7 * len(sentences))\n",
        "val_size = int(0.2 * len(sentences))\n",
        "test_size = int(0.1 * len(sentences))\n",
        "\n",
        "# Split into train, validation, and test sets\n",
        "train_sentences = sentences[:train_size]\n",
        "validation_sentences = sentences[train_size:train_size + val_size]\n",
        "test_sentences = sentences[train_size + val_size:train_size + val_size + test_size]\n",
        "\n",
        "# Print the number of tokenized sentences in each set\n",
        "print(f\"Number of tokenized train sentences: {len(train_sentences)}\")\n",
        "print(f\"Number of tokenized validation sentences: {len(validation_sentences)}\")\n",
        "print(f\"Number of tokenized test sentences: {len(test_sentences)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVfTk0FvM1E8"
      },
      "source": [
        "## Building vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cAk1AUSQ_2-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e98b59-c830-4eb7-b499-b129bbd46349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 22271\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    all_words = [word for sentence in sentences for word in sentence]\n",
        "    word_counts = Counter(all_words)\n",
        "    vocab = set(word_counts.keys())\n",
        "    return vocab\n",
        "\n",
        "all_sentences = train_sentences\n",
        "vocab = build_vocab(all_sentences)\n",
        "\n",
        "special_tokens = ['<UNK>', '<PAD>', '<S>', '</S>']\n",
        "for token in special_tokens:\n",
        "    vocab.add(token)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtm90Ok6hSGq"
      },
      "source": [
        "## Load Glove embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "o7dP_fp5hGgm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4766cad-9aee-4514-815d-1ca07074bc6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded GloVe embeddings with dimension: 300\n"
          ]
        }
      ],
      "source": [
        "embedding_dim = 300\n",
        "\n",
        "def load_glove_embeddings(glove_path):\n",
        "  word_embeddings = {}\n",
        "  with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        word_embeddings[word] = vector\n",
        "\n",
        "  special_tokens = ['<UNK>', '<PAD>', '<S>', '</S>']\n",
        "  for token in special_tokens:\n",
        "      if token in ['<S>', '</S>']:\n",
        "      #     # Random initialization for special tokens\n",
        "          word_embeddings[token] = np.random.rand(embedding_dim).astype('float32')\n",
        "      elif token == '<UNK>':\n",
        "          # Mean of all embeddings in GloVe for <UNK>\n",
        "          all_vectors = np.array(list(word_embeddings.values()))\n",
        "          mean_vector = np.mean(all_vectors, axis=0)\n",
        "          word_embeddings[token] = mean_vector\n",
        "      elif token == '<PAD>':\n",
        "          # Zero initialization for <PAD>\n",
        "          word_embeddings[token] = np.zeros(embedding_dim, dtype='float32')\n",
        "\n",
        "\n",
        "  return word_embeddings\n",
        "\n",
        "glove_path = '/content/drive/MyDrive/glove.6B.300d.txt'\n",
        "glove_embeddings = load_glove_embeddings(glove_path)\n",
        "\n",
        "# Check embedding dimensions\n",
        "embedding_dim = len(glove_embeddings[next(iter(glove_embeddings))])\n",
        "print(f\"Loaded GloVe embeddings with dimension: {embedding_dim}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJbyLrpDhP1s"
      },
      "source": [
        "## Create mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zx_tnfb3hPWL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79345233-657c-4fcb-8608-828b74c5a7cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample word2index mapping: [('bosom', 0), ('thistake', 1), ('irongray', 2), ('goodlooking', 3), ('twelvelet', 4), ('mall', 5), ('swept', 6), ('slavemerchants', 7), ('kingdom', 8), ('p.s.be', 9)]\n",
            "Sample index2word mapping: [(0, 'bosom'), (1, 'thistake'), (2, 'irongray'), (3, 'goodlooking'), (4, 'twelvelet'), (5, 'mall'), (6, 'swept'), (7, 'slavemerchants'), (8, 'kingdom'), (9, 'p.s.be')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22271, 22271, 400004, 4631)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "def build_mappings(vocab, glove_embeddings):\n",
        "    word2index = {word: idx for idx, word in enumerate(vocab)}\n",
        "    index2word = {idx: word for word, idx in word2index.items()}\n",
        "\n",
        "    return word2index, index2word\n",
        "\n",
        "word2index, index2word = build_mappings(vocab, glove_embeddings)\n",
        "\n",
        "print(f\"Sample word2index mapping: {list(word2index.items())[:10]}\")\n",
        "print(f\"Sample index2word mapping: {list(index2word.items())[:10]}\")\n",
        "\n",
        "count = 0;\n",
        "for word in word2index.keys():\n",
        "    if word not in glove_embeddings:\n",
        "        count = count + 1\n",
        "\n",
        "len(vocab), len(word2index), len(glove_embeddings), count"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pad_idx = word2index['<PAD>']\n",
        "pad_idx, index2word[pad_idx]"
      ],
      "metadata": {
        "id": "oaE-e1_XAsgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a78f3925-d06b-4e15-a8d3-05169248b1c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1766, '<PAD>')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxM2TumWx2nD"
      },
      "source": [
        "## Embeddings Matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 300\n",
        "vocab_size = len(word2index)\n",
        "embeddings_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for idx, word in index2word.items():\n",
        "    if word in glove_embeddings:\n",
        "        embeddings_matrix[idx] = glove_embeddings[word]\n",
        "    else:\n",
        "        unknown_embeds = glove_embeddings['<UNK>']\n",
        "        embeddings_matrix[idx] = unknown_embeds\n",
        "\n",
        "print(f\"Shape of embeddings matrix: {embeddings_matrix.shape}\")"
      ],
      "metadata": {
        "id": "AeBCeWWIY4zP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7288523-75b0-4d53-dc93-c8423a9b0ebd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of embeddings matrix: (22271, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Dataset"
      ],
      "metadata": {
        "id": "pNS1tLm6o_hY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9ZNoCs_BLkTI"
      },
      "outputs": [],
      "source": [
        "def get_index(word, word2index):\n",
        "    return word2index.get(word, word2index.get('<UNK>'))\n",
        "\n",
        "def create_context_label_dataset(sentences, index2word):\n",
        "    contexts = []\n",
        "    labels = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_indices = [get_index(word, word2index) for word in sentence]\n",
        "\n",
        "        # Create context (all words except the last) and label (all words except the first)\n",
        "        context = sentence_indices[:-1]\n",
        "        label = sentence_indices[1:]\n",
        "\n",
        "        contexts.append(context)\n",
        "        labels.append(label)\n",
        "\n",
        "    return contexts, labels\n",
        "\n",
        "\n",
        "train_contexts, train_labels = create_context_label_dataset(train_sentences, word2index)\n",
        "val_contexts, val_labels = create_context_label_dataset(validation_sentences, word2index)\n",
        "test_contexts, test_labels = create_context_label_dataset(test_sentences, word2index)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(8):\n",
        "  print(len(train_contexts[i]), len(train_labels[i]))"
      ],
      "metadata": {
        "id": "Sna4mcmfMi8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2581cd0b-ba80-46ed-853d-7897c0953661"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39 39\n",
            "29 29\n",
            "30 30\n",
            "179 179\n",
            "4 4\n",
            "5 5\n",
            "4 4\n",
            "4 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, contexts, labels):\n",
        "        self.contexts = contexts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.contexts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.contexts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "train_dataset = TextDataset(train_contexts, train_labels)\n",
        "validation_dataset = TextDataset(val_contexts, val_labels)\n",
        "test_dataset = TextDataset(test_contexts, test_labels)\n",
        "\n",
        "# collate function for padding\n",
        "def collate_fn(batch):\n",
        "    contexts, labels = zip(*batch)\n",
        "    # Pad sequences with '<PAD>' to make all sentences in the batch the same length\n",
        "    contexts_padded = pad_sequence(contexts, batch_first=True, padding_value=word2index['<PAD>'])\n",
        "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=word2index['<PAD>'])\n",
        "    return contexts_padded, labels_padded\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "SyH_S_VKNXwW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM class"
      ],
      "metadata": {
        "id": "kOao-BfRpFNf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "w6gFBFF9lEF5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d31376-80e3-4229-d6bf-a3553bec0e8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, pad_idx, embedding_matrix, embedding_dim, hidden_dim, output_dim, num_layers=1, dropout=0.4):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), padding_idx=pad_idx, freeze=True)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout, bidirectional=False)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        if hidden is None:\n",
        "            lstm_out, hidden = self.lstm(x)\n",
        "        else:\n",
        "            lstm_out, hidden = self.lstm(x, hidden)\n",
        "\n",
        "        # Detach hidden state to prevent backward through the graph multiple times\n",
        "        hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "        final_out = self.fc(self.dropout(lstm_out))\n",
        "        return final_out, hidden\n",
        "\n",
        "# Model params\n",
        "embedding_dim = 300\n",
        "hidden_dim = 300\n",
        "output_dim = len(vocab)\n",
        "num_layers = 1\n",
        "dropout = 0.3\n",
        "pad_idx = word2index['<PAD>']\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = LSTMModel(pad_idx, embeddings_matrix, embedding_dim, hidden_dim, output_dim, num_layers, dropout).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=0, factor=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tcMmx38Xlbr1"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs, patience):\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        # Initialize hidden state for the epoch\n",
        "        hidden = None\n",
        "\n",
        "        for batch_idx, (contexts, labels) in enumerate(train_loader):\n",
        "            contexts, labels = contexts.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch_size = contexts.size(0)  # Current batch size\n",
        "            if hidden is None or hidden[0].size(1) != batch_size:\n",
        "                hidden = None  # Reset hidden state if batch size is different\n",
        "\n",
        "            output, hidden = model(contexts, hidden)\n",
        "\n",
        "            # Reshape output and labels for CrossEntropyLoss\n",
        "            loss = criterion(output.view(-1, output.size(-1)), labels.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Train Loss: {avg_train_loss:.4f}')\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        hidden = None\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient calculation for validation\n",
        "            for batch_idx, (contexts, labels) in enumerate(val_loader):\n",
        "                contexts, labels = contexts.to(device), labels.to(device)\n",
        "\n",
        "                batch_size = contexts.size(0)\n",
        "                if hidden is None or hidden[0].size(1) != batch_size:\n",
        "                    hidden = None\n",
        "\n",
        "                # Forward pass through the model\n",
        "                output, hidden = model(contexts, hidden)\n",
        "\n",
        "                # Compute the validation loss\n",
        "                loss = criterion(output.view(-1, output.size(-1)), labels.view(-1))\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "        scheduler.step(avg_val_loss)\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), 'LSTM_model.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0o_75XvqlQMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ace06c0-9ea1-405f-a0dc-b27d1425aa2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Train Loss: 5.6287\n",
            "Validation Loss: 5.1324\n",
            "Epoch 2/10\n",
            "Train Loss: 4.9682\n",
            "Validation Loss: 4.8892\n",
            "Epoch 3/10\n",
            "Train Loss: 4.7369\n",
            "Validation Loss: 4.7650\n",
            "Epoch 4/10\n",
            "Train Loss: 4.5758\n",
            "Validation Loss: 4.6875\n",
            "Epoch 5/10\n",
            "Train Loss: 4.4456\n",
            "Validation Loss: 4.6348\n",
            "Epoch 6/10\n",
            "Train Loss: 4.3364\n",
            "Validation Loss: 4.6055\n",
            "Epoch 7/10\n",
            "Train Loss: 4.2404\n",
            "Validation Loss: 4.5811\n",
            "Epoch 8/10\n",
            "Train Loss: 4.1553\n",
            "Validation Loss: 4.5690\n",
            "Epoch 9/10\n",
            "Train Loss: 4.0784\n",
            "Validation Loss: 4.5699\n",
            "Epoch 10/10\n",
            "Train Loss: 3.9619\n",
            "Validation Loss: 4.5676\n"
          ]
        }
      ],
      "source": [
        "train_model(model, train_loader, validation_loader, criterion, optimizer, scheduler, device, num_epochs = 10, patience = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate model"
      ],
      "metadata": {
        "id": "StjisWjVpZVX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fRbKHUhaDYHG"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, device, criterion):\n",
        "      model.eval()  # Set model to evaluation mode\n",
        "      total_test_loss = 0\n",
        "      hidden = None\n",
        "\n",
        "      with torch.no_grad():  # Disable gradient calculation for validation\n",
        "          for batch_idx, (contexts, labels) in enumerate(test_loader):\n",
        "              contexts, labels = contexts.to(device), labels.to(device)\n",
        "\n",
        "              batch_size = contexts.size(0)\n",
        "              if hidden is None or hidden[0].size(1) != batch_size:\n",
        "                  hidden = None  # Reset hidden state if batch size is different\n",
        "\n",
        "              # Forward pass through the model\n",
        "              output, hidden = model(contexts, hidden)\n",
        "\n",
        "              # Compute the validation loss\n",
        "              loss = criterion(output.view(-1, output.size(-1)), labels.view(-1))\n",
        "              total_test_loss += loss.item()\n",
        "\n",
        "      avg_test_loss = total_test_loss / len(test_loader)\n",
        "\n",
        "    # Calculate perplexity\n",
        "      perplexity = math.exp(avg_test_loss)\n",
        "\n",
        "      return avg_test_loss, perplexity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, train_perplexity = evaluate_model(model, train_loader, device, criterion)\n",
        "val_loss, val_perplexity = evaluate_model(model, validation_loader, device, criterion)\n",
        "test_loss, test_perplexity = evaluate_model(model, test_loader, device, criterion)"
      ],
      "metadata": {
        "id": "Fc8pfNj3d_e5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train Loss: {train_loss:.4f}\")\n",
        "print(f\"Train Perplexity: {train_perplexity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YwOXZCGpgxr",
        "outputId": "285a7215-b207-40c7-94c4-3d7fae90d7df"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.8203\n",
            "Train Perplexity: 45.6201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Val Loss: {val_loss:.4f}\")\n",
        "print(f\"Val Perplexity: {val_perplexity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lc_f_AvpiK5",
        "outputId": "9ad5363a-2fa6-4125-d2b4-483fa7090391"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 4.5661\n",
            "Val Perplexity: 96.1638\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Perplexity: {test_perplexity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGhNtJ46pj9l",
        "outputId": "dcfe3846-50c3-4f86-b0d0-0f5d6092411f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 4.5510\n",
            "Test Perplexity: 94.7288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, path):\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "save_model(model,\"2021101102-LM2-LSTM.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y0bCBQrplS0",
        "outputId": "2364ef87-d31e-4bee-d004-9862e137db0c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to 2021101102-LM2-LSTM.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving perplexities to file"
      ],
      "metadata": {
        "id": "qUWj609Npr5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_perplexities(model, test_loader, file_name):\n",
        "\n",
        "    model.eval()\n",
        "    batch_perplexities = []\n",
        "    total_test_loss = 0\n",
        "    hidden = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (contexts, labels) in enumerate(test_loader):\n",
        "            contexts, labels = contexts.to(device), labels.to(device)\n",
        "\n",
        "            batch_size = contexts.size(0)\n",
        "            if hidden is None or hidden[0].size(1) != batch_size:\n",
        "                hidden = None  # Reset hidden state if batch size is different\n",
        "\n",
        "            # Forward pass through the model\n",
        "            output, hidden = model(contexts, hidden)\n",
        "\n",
        "            # Compute the validation loss\n",
        "            loss = criterion(output.view(-1, output.size(-1)), labels.view(-1))\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "    # Compute the average test loss and perplexity\n",
        "    avg_test_loss = total_test_loss / len(test_loader)\n",
        "    avg_perplexity = math.exp(avg_test_loss)\n",
        "\n",
        "    with open(file_name, 'a') as f:\n",
        "        f.write(f'Average Perplexity: {avg_perplexity:.4f}\\n')\n",
        "\n",
        "    # Print average perplexity\n",
        "    print(f'Average Perplexity: {avg_perplexity:.4f}')"
      ],
      "metadata": {
        "id": "SKuwhGaON38J"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"2021101102-LM2-train-perplexity.txt\"\n",
        "val_path = \"2021101102-LM2-val-perplexity.txt\"\n",
        "test_path = \"2021101102_LM2-test-perplexity.txt\"\n",
        "\n",
        "# Createa loaders with size 1\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "save_perplexities(model, train_loader, train_path)\n",
        "save_perplexities(model, validation_loader, val_path)\n",
        "save_perplexities(model, test_loader, test_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJKHh0Cup2Rd",
        "outputId": "5eea1d3f-02b4-4f8d-f3f7-d92ec3bc9c65"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Perplexity: 37.0503\n",
            "Average Perplexity: 74.7442\n",
            "Average Perplexity: 73.0540\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}